<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nccl on 夏田墨计算机笔记</title><link>https://javioustlj.github.io/cs-note/ai/nccl/</link><description>Recent content in nccl on 夏田墨计算机笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 14 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://javioustlj.github.io/cs-note/ai/nccl/index.xml" rel="self" type="application/rss+xml"/><item><title>concepts</title><link>https://javioustlj.github.io/cs-note/ai/nccl/concenpts/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/concenpts/</guid><description>concepts node: 一个服务器 CUDA device：一个GPU rank: GPU的编号 communicator: 这是一个抽象的概念，用于管理一组GPU之间的通信。 IB GDA temp full-blown
multi-GPU parallelization model:
single-threaded control of all GPUs multi-threaded, for example, using one thread per GPU multi-process, for example, MPI SHARP
NVLink Sharp IB Sharp Algorithm
NVLS IB Sharp</description></item><item><title>abbr</title><link>https://javioustlj.github.io/cs-note/ai/nccl/abbr/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/abbr/</guid><description> abbr english chinese mark SHARP Scalable Hierarchical Aggregation and Reduction Protocol CUDA Compute Unified Device Architecture NVLS NVLink Sharp IB InfiniBand SM Streaming Multiprocessor VMM Virtual Memory Management RAS Reliability, availability, Serviceability NIC Network Interface Card RDMA Remote Direct Memory Access GDRDMA GPU Direct RDMA DMA Direct Memory Access ACS Access Control Services IOMMU IO Memory Management Unit PLX ATS Address Translation Services RoCE RDMA over Converged Ethernet abbr english chinese mark NVTX NVIDIA Tools Extension</description></item><item><title/><link>https://javioustlj.github.io/cs-note/ai/nccl/bootstrapnetinit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/bootstrapnetinit/</guid><description>bootstrapNetInit 作用：找到可用的网络接口，并将它的信息保存下来。
如果配置了NCCL_COMM_ID，则使用和它在同一网段的网口。 ib 网络接口 普通网口 docker 网口 loopback 网口</description></item><item><title/><link>https://javioustlj.github.io/cs-note/ai/nccl/init/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/init/</guid><description>ncclCommInitAll ncclGetUniqueId ncclInit initEnv initGdrCopy bootstrapNetInit initNvtxRegisteredEnums bootstrapGetUniqueId 64位的随机数 + bootstrap网络接口地址
bootstrapCreateRoot ncclSocketInit 创建TCP socket ncclCommInitRankDev NCCLCHECKGOTO(ncclGroupStartInternal(), ret, fail); for (int i=0; i&amp;lt;ndev; i++) { // Ignore return codes .. we need to call ncclGroupEnd to clean up anyway int dev = devlist ? devlist[i] : i; CUDACHECKGOTO(cudaSetDevice(dev), ret, fail); ncclCommInitRankDev(comms+i, ndev,1, &amp;amp;uniqueId, i, dev, &amp;amp;config, __func__); }</description></item><item><title/><link>https://javioustlj.github.io/cs-note/ai/nccl/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/introduction/</guid><description>网络 nccl所需要两个网络，一个是bootstrap网络，一个是数据通信网络。 bootstrap网络主要用于初始化时交换一些简单的信息，如果每个机器的ip端口，由于数据量较小，而且只在初始化阶段执行一次，因此bootstrap使用的是tcp 而通信网络是用于实际数据的传输，因此优先使用RDMA（支持GDR的话会优先GDR）</description></item><item><title/><link>https://javioustlj.github.io/cs-note/ai/nccl/term/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://javioustlj.github.io/cs-note/ai/nccl/term/</guid><description>PCIe（高速串行总线） PCIe是一种连接扩展设备的高速串行总线标准。它是一种内部接口，用于连接主板（类似于购物中心）和各种插槽（类似于商店）上的扩展设备，如显卡、网卡、硬盘控制器和其他周边设备。PCIe提供了快速而可靠的数据传输通道，允许扩展设备与计算机系统进行高速通信。可以实现几十GB/S的传输速度
NVlink（NVIDIA的高速互连技术） NVLink是一种由NVIDIA开发的高速互连技术，用于在计算机系统中连接多个图形处理单元（GPU），以实现更高的数据传输速度和更低的延迟。能够提供比PCIe更快的传输速度，可通过桥接器连接两块NVIDIA显卡，实现上百GB/S的传输速度
Infiniband（无限带宽） InfiniBand（无限带宽）是一种高性能、低延迟的计算和存储网络架构和通信协议。 InfiniBand被广泛应用于高性能计算（HPC）领域和大规模数据中心，它提供了高带宽和低延迟的数据传输能力，适用于处理大规模并行计算、高速数据存储和集群应用。带宽可以达到400GB/S
MPI（并行计算Message Passing Interface） MPI (Message Passing Interface) 是一种用于编写并行程序的通信协议和标准。它提供了一套函数接口，使得在多个计算节点（通常是集群或超级计算机）上的多个进程之间进行消息传递和通信成为可能，从而实现并行计算。MPI主要用于解决并行计算中的通信和同步问题。它允许开发人员将一个问题分解为多个并行的任务，每个任务都在不同的计算节点上执行，通过消息传递来共享数据和结果
Pinned (page-locked) memory</description></item></channel></rss>